\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces $\mathbb {R}^3$ can be viewed as a subspace of quaternions called pure quaternions which have a real part of zero.\relax }}{10}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The architecture for LeNet-5, which is composed of repeating convolution and pooling layers.\relax }}{14}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Convolution operation performed on a single feature location and one convolution kernel \cite {convolution}.\relax }}{15}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Example max pooling operation with a size of 2x2. A 2x2 window is moved over the input with a stride of 2 and the maximum value of the window is taken \cite {maxpooling}.\relax }}{16}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Examples of some of the discussed activation functions. From top to bottom and left to right: ReLU, LReLU, PReLU, and ELU. \cite {}\relax }}{21}
\contentsline {figure}{\numberline {3.2}{\ignorespaces An inception block from \cite {szegedy2015going}. Notice the 1x1 convolutions before the 3x3 and 5x5 convolutions are reducing the number of feature channels. This reduces the number of multiplications that must be performed.\relax }}{23}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A residual block from \cite {he2015deep}.\relax }}{24}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Backwards convolution with a stride of 2 upsampling a 3x3 image to a 5x5 image. Notice the 3x3 image is padded with zeros, the white tiles. The shaded grey area is the actual convolution kernel, which is learned during training.\relax }}{25}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Transforming fully connected layers into convolution layers enables a classification net to output a heatmap \cite {long2015fully}. This is achieved by using 1x1 convolutions instead of fully connected layers and will produce a tiny heatmap of classes. This change also make the network become independent of input image size.\relax }}{26}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Three different architectures from \cite {long2015fully}. The first uses no short connections, the second forwards information from pool4, and the third forwards information from both pool3 and pool4.\relax }}{27}
\contentsline {figure}{\numberline {3.7}{\ignorespaces U-Net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations. \cite {ronneberger2015u}.\relax }}{28}
\contentsline {figure}{\numberline {3.8}{\ignorespaces One iteration of the RNN represented as common neural network layers. The block approximates $\delta u_{k,t}$ and then adds it to $u_{k,t}$ to estimate $\delta u_{k,t}$. \cite {maggiori2016learning}\relax }}{30}
\contentsline {figure}{\numberline {3.9}{\ignorespaces The whole RNN shown by multiple blocks from Fig.3.8\hbox {} with shared weights. \cite {maggiori2016learning}.\relax }}{30}
\contentsline {figure}{\numberline {3.10}{\ignorespaces GAN formulation of semantic segmentation from \cite {luc2016semantic}. Left: segmentation net takes RGB image as input, and produces per-pixel class predictions. Right: Adversarial net takes label map as input and produces class label (1=ground truth, or 0=synthetic).\relax }}{33}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces The model architecture. Feature maps from the down path get summed with feature maps of the corresponding size in the up path. The ResBlock types are shown in Figs.\nobreakspace {}4.2a\hbox {}\nobreakspace {}\&\nobreakspace {}4.2b\hbox {} and the DConvolution and UConvolution are shown in Fig.\nobreakspace {}4.2c\hbox {} and Fig.\nobreakspace {}4.2d\hbox {} respectively.\relax }}{38}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Model components.\relax }}{39}
\contentsline {figure}{\numberline {4.3}{\ignorespaces The loss and validation loss during training for the baseline models.\relax }}{40}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Sample segmentation on a test image for the baseline models. The test image is shown in its normalize stated, which is scaling the values between minus one and one. From left to right: test image, U-Net segmentation, our model, and our model with multi-scale convolution. The segmentations are shown in a red to blue color map where red is 0 and blue is 1.\relax }}{41}
\contentsline {figure}{\numberline {4.5}{\ignorespaces The loss and validation loss during training for the batchnorm models.\relax }}{42}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Sample segmentation on a test image for the batchnorm models. From left to right: test image, U-Net segmentation, our model, and our model with multi-scale convolution.\relax }}{42}
\addvspace {10\p@ }
\addvspace {10\p@ }
