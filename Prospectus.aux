\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Quaternions}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Quaternion Algebra}{6}}
\newlabel{s:quatalg}{{1.1}{6}}
\newlabel{eq:quaternion1}{{1.1}{6}}
\newlabel{eq:quarternion2}{{1.2}{7}}
\newlabel{eq:quarternion3}{{1.3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Addition and Multiplication}{7}}
\newlabel{eq:q}{{1.4}{7}}
\newlabel{eq:p}{{1.5}{7}}
\newlabel{eq:quataddition}{{1.6}{7}}
\newlabel{eq:quatmult}{{1.7}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Conjugate, Norm, and Inverse}{8}}
\newlabel{eq:quatconjugate}{{1.8}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Geometric Representation of Quaternions}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Complex Algebra}{8}}
\newlabel{eq:complexalgebra}{{1.9}{8}}
\newlabel{eq:complexaddition}{{1.10}{9}}
\newlabel{eq:complexmult}{{1.11}{9}}
\newlabel{eq:complexlength}{{1.12}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Complex Rotation Operation}{9}}
\newlabel{eq:euler}{{1.13}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Quaternion Rotation Operation}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces $\mathbb  {R}^3$ can be viewed as a subspace of quaternions called pure quaternions which have a real part of zero.\relax }}{10}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{f:r3}{{1.1}{10}}
\newlabel{eq:Lq}{{1.14}{10}}
\newlabel{eq:unitquat}{{1.15}{11}}
\newlabel{eq:p11}{{1.16}{11}}
\citation{hubel1968receptive}
\citation{lecun1990handwritten}
\citation{hecht1988theory}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Introduction To Convolutional Neural Networks}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Basic CNN Components}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The architecture for LeNet-5, which is composed of repeating convolution and pooling layers.\relax }}{14}}
\newlabel{f:lenet}{{2.1}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Convolutional Layer}{14}}
\newlabel{eq:convbasic}{{2.1}{14}}
\citation{convolution}
\citation{convolution}
\newlabel{eq:activationbasic}{{2.2}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Convolution operation performed on a single feature location and one convolution kernel \cite  {convolution}.\relax }}{15}}
\newlabel{f:convolution}{{2.2}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Pooling Layer}{15}}
\citation{maxpooling}
\citation{maxpooling}
\citation{rumelhart1985learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example max pooling operation with a size of 2x2. A 2x2 window is moved over the input with a stride of 2 and the maximum value of the window is taken \cite  {maxpooling}.\relax }}{16}}
\newlabel{f:maxpool}{{2.3}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}CNN Common Tasks}{16}}
\citation{maldonado2007road}
\citation{li2015automatic,lyksborg2015ensemble,kainz2015semantic,havaei2017brain}
\citation{chen2013vehicle}
\citation{du2016fused}
\citation{hochreiter2001gradient}
\citation{nair2010rectified}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}State of the Art}{18}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{StateOfTheArt}{{3}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Activation Functions}{18}}
\citation{glorot2011deep,krizhevsky2012imagenet,zeiler2013rectified,maas2013rectifier}
\citation{maas2013rectifier}
\citation{he2015delving}
\citation{clevert2015fast}
\@writefile{toc}{\contentsline {paragraph}{ReLU:}{19}}
\newlabel{e:relu}{{3.1}{19}}
\@writefile{toc}{\contentsline {paragraph}{LReLU:}{19}}
\newlabel{e:lrelu}{{3.2}{19}}
\@writefile{toc}{\contentsline {paragraph}{PReLU:}{19}}
\newlabel{e:prelu}{{3.3}{19}}
\citation{trottier2016parametric}
\citation{trottier2016parametric}
\citation{}
\citation{}
\citation{hinton2012improving}
\citation{wang2013fast}
\citation{ba2013adaptive}
\citation{tompson2015efficient}
\@writefile{toc}{\contentsline {paragraph}{ELU:}{20}}
\newlabel{e:elu}{{3.4}{20}}
\@writefile{toc}{\contentsline {paragraph}{PELU:}{20}}
\newlabel{e:pelu}{{3.5}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Regularization}{20}}
\@writefile{toc}{\contentsline {paragraph}{Dropout:}{20}}
\newlabel{eq:dropout}{{3.6}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Examples of some of the discussed activation functions. From top to bottom and left to right: ReLU, LReLU, PReLU, and ELU. \cite  {}\relax }}{21}}
\newlabel{f:activations}{{3.1}{21}}
\citation{ioffe2015batch}
\citation{lin2013network}
\@writefile{toc}{\contentsline {paragraph}{Batch Normalization:}{22}}
\newlabel{eq:BN1}{{3.7}{22}}
\newlabel{eq:BN2}{{3.8}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Architecture Improvements}{22}}
\citation{szegedy2015going}
\citation{szegedy2015going}
\citation{szegedy2015going}
\citation{he2015deep}
\citation{he2015deep}
\citation{he2015deep}
\@writefile{toc}{\contentsline {paragraph}{Network-In-Network}{23}}
\@writefile{toc}{\contentsline {paragraph}{Inception Blocks}{23}}
\@writefile{toc}{\contentsline {paragraph}{Residual Connections}{23}}
\citation{trabelsi2017deep}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An inception block from \cite  {szegedy2015going}. Notice the 1x1 convolutions before the 3x3 and 5x5 convolutions are reducing the number of feature channels. This reduces the number of multiplications that must be performed.\relax }}{24}}
\newlabel{f:inceptionblock}{{3.2}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A residual block from \cite  {he2015deep}.\relax }}{24}}
\newlabel{f:resblock}{{3.3}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Open Questions}{25}}
\citation{arjovsky2016unitary,danihelka2016associative,wisdom2016full}
\citation{trabelsi2017deep}
\citation{bulow1999hypercomplex,sangwine2000colour,bulow2001hypercomplex}
\citation{rishiyur2006neural,kendall2015posenet}
\citation{parcollet2016quaternion}
\citation{minemoto2017feed}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Work Completed}{26}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{26}}
\citation{trabelsi2017deep}
\citation{kendall2015posenet}
\citation{oppenheim1981importance}
\citation{bulow2001hypercomplex}
\citation{sangwine2000colour}
\citation{shi2007quaternion}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Motivation and Related Work}{27}}
\citation{minemoto2017feed}
\citation{trabelsi2017deep}
\citation{bulow1999hypercomplex,sangwine2000colour,bulow2001hypercomplex}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Quaternion Network Components}{28}}
\citation{trabelsi2017deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Quaternion Representation}{29}}
\newlabel{eq:m4r}{{4.1}{29}}
\citation{chollet2016xception}
\citation{shi2007quaternion}
\citation{ioffe2015batch}
\citation{trabelsi2017deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Quaternion Differentiability}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Quaternion Convolution}{30}}
\newlabel{s:qc}{{4.3.3}{30}}
\newlabel{eq:convolve1}{{4.2}{30}}
\newlabel{eq:}{{4.3}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces An illustration of quaternion convolution.\relax }}{31}}
\newlabel{f:quatconv}{{4.1}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Quaternion Batch-Normalization}{32}}
\newlabel{eq:white4d}{{4.4}{32}}
\newlabel{eq:V4d}{{4.5}{33}}
\newlabel{eq:gamma}{{4.6}{33}}
\citation{glorot2010understanding}
\citation{he2015delving}
\citation{turner2002}
\citation{glorot2010understanding}
\citation{he2015delving}
\citation{nair2010rectified}
\newlabel{eq:qbn}{{4.7}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Quaternion Weight Initialization}{34}}
\newlabel{eq:quaternion_weight}{{4.8}{34}}
\newlabel{eq:variance}{{4.9}{34}}
\newlabel{eq:expected}{{4.10}{34}}
\newlabel{eq:variance_sigma}{{4.11}{34}}
\citation{trabelsi2017deep}
\citation{he2016deep}
\citation{trabelsi2017deep}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Experimental Results}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Classification}{35}}
\citation{nesterov1983method}
\citation{trabelsi2017deep}
\citation{he2016deep}
\citation{trabelsi2017deep}
\citation{he2016deep}
\citation{trabelsi2017deep}
\citation{he2016deep}
\citation{trabelsi2017deep}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Classification error on CIFAR-10 and CIFAR-100. Note that \cite  {he2016deep} is a 110 layer residual network, \cite  {trabelsi2017deep} is 118 layer complex network with the same design as the prior except with additional initial layers to extract complex mappings.\relax }}{36}}
\newlabel{t:results1}{{4.1}{36}}
\citation{harchaoui2017deep}
\citation{qi2017pointnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Segmentation}{37}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces IOU on KITTI Road Estimation benchmark.\relax }}{37}}
\newlabel{t:results2}{{4.2}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusions and Proposed Research}{37}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Appendix}{39}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}The Generalized Quaternion Chain Rule for a Real-Valued Function}{39}}
\newlabel{a:diff}{{5.1}{39}}
\newlabel{eq:diff1}{{5.2}{40}}
\citation{kessy2017optimal}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Whitening a Matrix}{41}}
\newlabel{a:whitening}{{5.2}{41}}
\newlabel{eq:white1}{{5.3}{41}}
\newlabel{eq:white2}{{5.4}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Cholesky Decomposition}{41}}
\newlabel{eq:cholesky1}{{5.5}{41}}
\bibstyle{alpha}
\bibdata{bib}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}4 DOF Independent Normal Distribution}{42}}
\newlabel{eq:single_dists}{{5.6}{42}}
\newlabel{eq:cumdist}{{5.7}{42}}
\newlabel{eq:4dsphere}{{5.8}{42}}
\newlabel{eq:polarint}{{5.9}{42}}
\newlabel{eq:finaldist}{{5.10}{42}}
\@writefile{toc}{\vspace *{\baselineskip }}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{42}}
\bibcite{convolution}{App00}
\bibcite{arjovsky2016unitary}{ASB16}
\bibcite{ba2013adaptive}{BF13}
\bibcite{bulow2001hypercomplex}{BS01}
\bibcite{bulow1999hypercomplex}{B{\"u}l99}
\bibcite{chollet2016xception}{Cho16}
\bibcite{clevert2015fast}{CUH15}
\bibcite{chen2013vehicle}{CXLP13}
\bibcite{du2016fused}{DEKLD16}
\bibcite{danihelka2016associative}{DWU{$^{+}$}16}
\bibcite{glorot2010understanding}{GB10}
\bibcite{glorot2011deep}{GBB11}
\bibcite{hochreiter2001gradient}{HBF{$^{+}$}01}
\bibcite{havaei2017brain}{HDWF{$^{+}$}17}
\bibcite{harchaoui2017deep}{HMB17}
\bibcite{hecht1988theory}{HN{$^{+}$}88}
\bibcite{hinton2012improving}{HSK{$^{+}$}12}
\bibcite{hubel1968receptive}{HW68}
\bibcite{he2015deep}{HZRS15a}
\bibcite{he2015delving}{HZRS15b}
\bibcite{he2016deep}{HZRS16}
\bibcite{ioffe2015batch}{IS15}
\bibcite{maxpooling}{Kar13}
\bibcite{kendall2015posenet}{KGC15}
\bibcite{kessy2017optimal}{KLS17}
\bibcite{kainz2015semantic}{KPU15}
\bibcite{krizhevsky2012imagenet}{KSH12}
\bibcite{lecun1990handwritten}{LBD{$^{+}$}90}
\bibcite{lin2013network}{LCY13}
\bibcite{li2015automatic}{LJH15}
\bibcite{lyksborg2015ensemble}{LPAL15}
\bibcite{maldonado2007road}{MBLAGJ{$^{+}$}07}
\bibcite{maas2013rectifier}{MHN13}
\bibcite{minemoto2017feed}{MINM17}
\bibcite{nesterov1983method}{Nes}
\bibcite{nair2010rectified}{NH10}
\bibcite{oppenheim1981importance}{OL81}
\bibcite{parcollet2016quaternion}{PMB{$^{+}$}16}
\bibcite{turner2002}{PT02}
\bibcite{qi2017pointnet}{QSMG17}
\bibcite{rumelhart1985learning}{RHW85}
\bibcite{rishiyur2006neural}{Ris06}
\bibcite{sangwine2000colour}{SE00}
\bibcite{shi2007quaternion}{SF07}
\bibcite{szegedy2015going}{SLJ{$^{+}$}15}
\bibcite{trabelsi2017deep}{TBS{$^{+}$}17}
\bibcite{trottier2016parametric}{TGCd16}
\bibcite{tompson2015efficient}{TGJ{$^{+}$}15}
\bibcite{wang2013fast}{WM13}
\bibcite{wisdom2016full}{WPH{$^{+}$}16}
\bibcite{zeiler2013rectified}{ZRM{$^{+}$}13}
\citation{convolution}
\citation{maxpooling}
\citation{}
\citation{szegedy2015going}
\citation{he2015deep}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{49}}
\citation{he2016deep}
\citation{trabelsi2017deep}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{50}}
