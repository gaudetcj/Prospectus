\relax 
\citation{trabelsi2017deep}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Deep Learning}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Beyond Real Numbers}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Outline}{7}}
\citation{hubel1968receptive}
\citation{lecun1990handwritten}
\citation{hecht1988theory}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Introduction To Convolutional Neural Networks}{8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Basic CNN Components}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The architecture for LeNet-5, which is composed of repeating convolution and pooling layers.\relax }}{9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{f:lenet}{{2.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Convolutional Layer}{9}}
\newlabel{eq:convbasic}{{2.1}{9}}
\citation{convolution}
\citation{convolution}
\newlabel{eq:activationbasic}{{2.2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Convolution operation performed on a single feature location and one convolution kernel \cite  {convolution}.\relax }}{10}}
\newlabel{f:convolution}{{2.2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Pooling Layer}{10}}
\citation{maxpooling}
\citation{maxpooling}
\citation{rumelhart1985learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example max pooling operation with a size of 2x2. A 2x2 window is moved over the input with a stride of 2 and the maximum value of the window is taken \cite  {maxpooling}.\relax }}{11}}
\newlabel{f:maxpool}{{2.3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}CNN Common Tasks}{11}}
\citation{maldonado2007road}
\citation{li2015automatic,lyksborg2015ensemble,kainz2015semantic,havaei2017brain}
\citation{chen2013vehicle}
\citation{du2016fused}
\citation{hochreiter2001gradient}
\citation{nair2010rectified}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}State of the Art}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{StateOfTheArt}{{3}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Activation Functions}{13}}
\citation{glorot2011deep,krizhevsky2012imagenet,zeiler2013rectified,maas2013rectifier}
\citation{maas2013rectifier}
\citation{he2015delving}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}ReLU:}{14}}
\newlabel{e:relu}{{3.1}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}LReLU:}{14}}
\newlabel{e:lrelu}{{3.2}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}PReLU:}{14}}
\citation{clevert2015fast}
\citation{trottier2016parametric}
\citation{trottier2016parametric}
\citation{glorot2010understanding}
\citation{he2015delving}
\newlabel{e:prelu}{{3.3}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}ELU:}{15}}
\newlabel{e:elu}{{3.4}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}PELU:}{15}}
\newlabel{e:pelu}{{3.5}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Examples of some of the discussed activation functions. From top to bottom and left to right: ReLU, LReLU, PReLU, and ELU.\relax }}{16}}
\newlabel{f:activations}{{3.1}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Weight Initialization}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Glorot Initialization}{17}}
\newlabel{eq:glorotinit}{{3.6}{17}}
\citation{hinton2012improving}
\citation{wang2013fast}
\citation{ba2013adaptive}
\citation{tompson2015efficient}
\newlabel{eq:glorotshort}{{3.7}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}He Initialization}{18}}
\newlabel{eq:heinit}{{3.8}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Regularization}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Dropout}{18}}
\newlabel{eq:dropout}{{3.9}{18}}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Batch Normalization}{19}}
\newlabel{eq:BN1}{{3.10}{19}}
\newlabel{eq:BN2}{{3.11}{19}}
\citation{lin2013network}
\citation{szegedy2015going}
\citation{szegedy2015going}
\citation{szegedy2015going}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Architecture Improvements}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Network-In-Network}{20}}
\citation{he2015deep}
\citation{he2015deep}
\citation{he2015deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Inception Blocks}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An inception block from \cite  {szegedy2015going}. Notice the 1x1 convolutions before the 3x3 and 5x5 convolutions are reducing the number of feature channels. This reduces the number of multiplications that must be performed.\relax }}{21}}
\newlabel{f:inceptionblock}{{3.2}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Residual Connections}{21}}
\citation{trabelsi2017deep}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A residual block from \cite  {he2015deep}.\relax }}{22}}
\newlabel{f:resblock}{{3.3}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Conclusion}{22}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Quaternions}{24}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Quaternion Algebra}{24}}
\newlabel{s:quatalg}{{4.1}{24}}
\newlabel{eq:quaternion1}{{4.1}{24}}
\citation{hungerford1980algebra}
\newlabel{eq:quarternion2}{{4.2}{25}}
\newlabel{eq:quarternion3}{{4.3}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Addition and Multiplication}{25}}
\newlabel{eq:q}{{4.4}{25}}
\newlabel{eq:p}{{4.5}{25}}
\newlabel{eq:quataddition}{{4.6}{25}}
\newlabel{eq:quatmult}{{4.7}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Conjugate, Norm, and Inverse}{26}}
\newlabel{eq:quatconjugate}{{4.8}{26}}
\newlabel{eq:quatnorm}{{4.1.2}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Geometric Representation of Quaternions}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Complex Algebra}{27}}
\newlabel{eq:complexalgebra}{{4.9}{27}}
\newlabel{eq:complexaddition}{{4.10}{27}}
\citation{jia2008quaternions}
\citation{jia2008quaternions}
\newlabel{eq:complexmult}{{4.11}{28}}
\newlabel{eq:complexlength}{{4.12}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Complex Rotation Operation}{28}}
\newlabel{eq:euler}{{4.13}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Quaternion Rotation Operation}{28}}
\citation{hungerford1980algebra}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces $\mathbb  {R}^3$ can be viewed as a subspace of quaternions called pure quaternions which have a real part of zero. \cite  {jia2008quaternions}\relax }}{29}}
\newlabel{f:r3}{{4.1}{29}}
\newlabel{eq:Lq}{{4.14}{29}}
\citation{jia2008quaternions}
\newlabel{eq:unitquat}{{4.15}{30}}
\newlabel{eq:p11}{{4.16}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Conclusion}{31}}
\newlabel{s:quatConc}{{4.3}{31}}
\citation{kendall2015posenet}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Quaternion Networks}{32}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Motivation and Related Work}{32}}
\citation{oppenheim1981importance}
\citation{bulow2001hypercomplex}
\citation{sangwine2000colour}
\citation{shi2007quaternion}
\citation{minemoto2017feed}
\citation{trabelsi2017deep}
\citation{bulow1999hypercomplex,sangwine2000colour,bulow2001hypercomplex}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Quaternion Network Components}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Quaternion Representation}{34}}
\citation{trabelsi2017deep}
\citation{chollet2016xception}
\citation{shi2007quaternion}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Quaternion Differentiability}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Quaternion Convolution}{35}}
\newlabel{s:qc}{{5.2.3}{35}}
\newlabel{eq:m4r}{{5.1}{35}}
\newlabel{eq:qconvolve2}{{5.2}{35}}
\citation{ioffe2015batch}
\citation{trabelsi2017deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Quaternion Batch-Normalization}{36}}
\newlabel{eq:white4d}{{5.3}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces An illustration of quaternion convolution.\relax }}{37}}
\newlabel{f:quatconv}{{5.1}{37}}
\citation{glorot2010understanding}
\citation{he2015delving}
\newlabel{eq:gamma}{{5.4}{38}}
\newlabel{eq:qbn}{{5.5}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Quaternion Weight Initialization}{38}}
\citation{turner2002}
\citation{glorot2010understanding}
\citation{he2015delving}
\citation{nair2010rectified}
\newlabel{eq:quaternion_weight}{{5.6}{39}}
\newlabel{eq:variance}{{5.7}{39}}
\newlabel{eq:expected}{{5.8}{39}}
\newlabel{eq:variance_sigma}{{5.9}{39}}
\citation{trabelsi2017deep}
\citation{he2016deep}
\citation{trabelsi2017deep}
\citation{nesterov1983method}
\citation{trabelsi2017deep}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Experimental Results}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Classification}{40}}
\citation{he2016deep}
\citation{trabelsi2017deep}
\citation{he2016deep}
\citation{trabelsi2017deep}
\citation{he2016deep}
\citation{trabelsi2017deep}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Classification error on CIFAR-10 and CIFAR-100. Note that \cite  {he2016deep} is a 110 layer residual network, \cite  {trabelsi2017deep} is 118 layer complex network with the same design as the prior except with additional initial layers to extract complex mappings.\relax }}{41}}
\newlabel{t:results1}{{5.1}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Segmentation}{41}}
\citation{harchaoui2017deep}
\citation{qi2017pointnet}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces IOU on KITTI Road Estimation benchmark.\relax }}{42}}
\newlabel{t:results2}{{5.2}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Conclusions and Proposed Research}{42}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Appendix}{43}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}The Generalized Quaternion Chain Rule for a Real-Valued Function}{43}}
\newlabel{a:diff}{{6.1}{43}}
\newlabel{eq:diff1}{{6.2}{44}}
\citation{kessy2017optimal}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Whitening a Matrix}{45}}
\newlabel{a:whitening}{{6.2}{45}}
\newlabel{eq:white1}{{6.3}{45}}
\newlabel{eq:white2}{{6.4}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Cholesky Decomposition}{45}}
\newlabel{eq:cholesky1}{{6.5}{45}}
\bibstyle{alpha}
\bibdata{bib}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}4 DOF Independent Normal Distribution}{46}}
\newlabel{eq:single_dists}{{6.6}{46}}
\newlabel{eq:cumdist}{{6.7}{46}}
\newlabel{eq:4dsphere}{{6.8}{46}}
\newlabel{eq:polarint}{{6.9}{46}}
\newlabel{eq:finaldist}{{6.10}{46}}
\@writefile{toc}{\vspace *{\baselineskip }}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{46}}
\bibcite{convolution}{App00}
\bibcite{ba2013adaptive}{BF13}
\bibcite{bulow2001hypercomplex}{BS01}
\bibcite{bulow1999hypercomplex}{B{\"u}l99}
\bibcite{chollet2016xception}{Cho16}
\bibcite{clevert2015fast}{CUH15}
\bibcite{chen2013vehicle}{CXLP13}
\bibcite{du2016fused}{DEKLD16}
\bibcite{glorot2010understanding}{GB10}
\bibcite{glorot2011deep}{GBB11}
\bibcite{hochreiter2001gradient}{HBF{$^{+}$}01}
\bibcite{havaei2017brain}{HDWF{$^{+}$}17}
\bibcite{harchaoui2017deep}{HMB17}
\bibcite{hecht1988theory}{HN{$^{+}$}88}
\bibcite{hinton2012improving}{HSK{$^{+}$}12}
\bibcite{hungerford1980algebra}{Hun80}
\bibcite{hubel1968receptive}{HW68}
\bibcite{he2015deep}{HZRS15a}
\bibcite{he2015delving}{HZRS15b}
\bibcite{he2016deep}{HZRS16}
\bibcite{ioffe2015batch}{IS15}
\bibcite{jia2008quaternions}{Jia08}
\bibcite{maxpooling}{Kar13}
\bibcite{kendall2015posenet}{KGC15}
\bibcite{kessy2017optimal}{KLS17}
\bibcite{kainz2015semantic}{KPU15}
\bibcite{krizhevsky2012imagenet}{KSH12}
\bibcite{lecun1990handwritten}{LBD{$^{+}$}90}
\bibcite{lin2013network}{LCY13}
\bibcite{li2015automatic}{LJH15}
\bibcite{lyksborg2015ensemble}{LPAL15}
\bibcite{maldonado2007road}{MBLAGJ{$^{+}$}07}
\bibcite{maas2013rectifier}{MHN13}
\bibcite{minemoto2017feed}{MINM17}
\bibcite{nesterov1983method}{Nes}
\bibcite{nair2010rectified}{NH10}
\bibcite{oppenheim1981importance}{OL81}
\bibcite{turner2002}{PT02}
\bibcite{qi2017pointnet}{QSMG17}
\bibcite{rumelhart1985learning}{RHW85}
\bibcite{sangwine2000colour}{SE00}
\bibcite{shi2007quaternion}{SF07}
\bibcite{szegedy2015going}{SLJ{$^{+}$}15}
\bibcite{trabelsi2017deep}{TBS{$^{+}$}17}
\bibcite{trottier2016parametric}{TGCd16}
\bibcite{tompson2015efficient}{TGJ{$^{+}$}15}
\bibcite{wang2013fast}{WM13}
\bibcite{zeiler2013rectified}{ZRM{$^{+}$}13}
\citation{convolution}
\citation{maxpooling}
\citation{szegedy2015going}
\citation{he2015deep}
\citation{jia2008quaternions}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{53}}
\citation{he2016deep}
\citation{trabelsi2017deep}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{54}}
